{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cweMlOB0L4mG",
        "Po-TEvrWMJ_a"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Author : Désiré OUEDRAOGO"
      ],
      "metadata": {
        "id": "g9qPct-D55eU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwd38XvZK2sy"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/NVIDIA/cuda-samples.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3ABSPkaBBHB",
        "outputId": "7ab46723-ebd4-462f-c713-686432e684ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cuda-samples'...\n",
            "remote: Enumerating objects: 12395, done.\u001b[K\n",
            "remote: Counting objects: 100% (12395/12395), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1905/1905), done.\u001b[K\n",
            "remote: Total 12395 (delta 10510), reused 12332 (delta 10464), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (12395/12395), 130.29 MiB | 12.75 MiB/s, done.\n",
            "Resolving deltas: 100% (10510/10510), done.\n",
            "Updating files: 100% (3673/3673), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ewR7_sHJ00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf88a85-8c8e-417c-e3fa-8e2cad468059"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Tue_Mar__8_18:18:20_PST_2022\n",
            "Cuda compilation tools, release 11.6, V11.6.124\n",
            "Build cuda_11.6.r11.6/compiler.31057947_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUIyq5vPjxKc"
      },
      "source": [
        "Based on the lecture at https://sites.google.com/site/frehseg/teaching/ia307"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cweMlOB0L4mG"
      },
      "source": [
        "# Provided Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po-TEvrWMJ_a"
      },
      "source": [
        "## CUDA Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-lgwhE1N5_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2b2a7b-7cb6-4665-cae6-dbd5b9cbe6c8"
      },
      "source": [
        "%%writefile cuda_stuff.cuh\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "#ifndef cuda_stuff_H\n",
        "#define cuda_stuff_H\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major, \n",
        "   nb_rows = number of rows */\n",
        "#define IDX2C(i,j,nb_rows) (((j)*(nb_rows))+(i))\n",
        " \n",
        "//MACRO TO DEBUGG CUDA FUNCTIONS\n",
        "/** Error checking,\n",
        " *  taken from https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api\n",
        " */\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess) \n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "/** Error checking for use with CUDA Dynamic Parallelism */\n",
        "/*\n",
        "#define cdpErrchk(ans) { cdpAssert((ans), __FILE__, __LINE__); }\n",
        "__device__ void cdpAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      printf(\"GPU kernel assert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) assert(0);\n",
        "   }\n",
        "}\n",
        "*/\n",
        "void device_synchronize();\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_stuff.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iivrxLaYOYPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75bea91b-d831-453d-d4d0-fde708e57a71"
      },
      "source": [
        "%%writefile cuda_stuff.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include \"cuda_stuff.cuh\"\n",
        "\n",
        "void device_synchronize(){\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_stuff.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fsEMpauK8lW"
      },
      "source": [
        "## fmatrix Matrix Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A97U902HMog4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba537d9-c1cb-455b-f697-0b88a7b690c3"
      },
      "source": [
        "%%writefile fmatrix.cuh\n",
        "#ifndef fmatrices_H\n",
        "#define fmatrices_H\n",
        "#include \"cuda_stuff.cuh\" // for IDX2C\n",
        "\n",
        "typedef struct {\n",
        "    float* data;\n",
        "    int cols;\n",
        "    int rows;\n",
        "} fmatrix;\n",
        "\n",
        "/* Access element (i,j) of matrix mat */\n",
        "#define getfm(mat,i,j) (mat.data[IDX2C(i,j,mat.rows)])\n",
        "\n",
        "\n",
        "int fmatrix_elements(fmatrix mat);\n",
        "int fmatrix_size(fmatrix mat);\n",
        "/** Assert that the matrix is coherent: all fields nonzero. */\n",
        "void fmatrix_assert(fmatrix mat);\n",
        "\n",
        "fmatrix fmatrix_create_on_host(int rows, int cols);\n",
        "fmatrix fmatrix_create_on_device(int rows, int cols);\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device);\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device);\n",
        "fmatrix fmatrix_copy_to_host(fmatrix mat_host);\n",
        "fmatrix fmatrix_copy_to_device(fmatrix mat_host);\n",
        "void fmatrix_free_on_host(fmatrix* mat);\n",
        "void fmatrix_free_on_device(fmatrix* mat);\n",
        "\n",
        "/** Create a matrix representing columns [a,b) of M. \n",
        " *  Note that the new matrix points into the\n",
        " *  data of M. The data is not copied to a new location.\n",
        " */\n",
        "fmatrix fmatrix_subcolumns(fmatrix M, int a, int b);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the host. \n",
        " *  If nb<0, print all rows. \n",
        " *\n",
        " * @attention: Works in device code if mat is on the device\n",
        " * and works in host code if mat is on the host.\n",
        " * All other cases require transferring the data first. \n",
        " */\n",
        " __host__\n",
        " __device__\n",
        "void fmatrix_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the device. \n",
        " *  If nb<0, print all rows. \n",
        " *\n",
        " *  This version copies the matrix to host first.\n",
        " */\n",
        "void fmatrix_device_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fmatrix.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGwZ36ifWQ-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef8373b-f136-4acb-e28b-a6731388e3fa"
      },
      "source": [
        "%%writefile fmatrix.cu\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "int fmatrix_elements(fmatrix mat) {\n",
        "     return mat.cols*mat.rows;\n",
        "}\n",
        "\n",
        "int fmatrix_size(fmatrix mat) {\n",
        "    // the following is only valid if matrix data already allocated,\n",
        "    // so it can't be used in create_on_... functions\n",
        "    // return fmatrix_elements(mat) * sizeof(mat.data[0]);\n",
        "    return fmatrix_elements(mat) * sizeof(float);\n",
        "}\n",
        "\n",
        "void fmatrix_assert(fmatrix mat) {\n",
        "    assert(mat.data);\n",
        "    assert(mat.cols);\n",
        "    assert(mat.rows);\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_host(int rows, int cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    mat.data = (float*)malloc(fmatrix_size(mat)); \n",
        "    assert(mat.data);\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_device(int rows, int cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    gpuErrchk( \n",
        "        cudaMalloc((void **)&(mat.data), fmatrix_size(mat)) \n",
        "    );\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk( \n",
        "        cudaMemcpy( mat_device.data, mat_host.data, \n",
        "                   fmatrix_size(mat_host), \n",
        "                   cudaMemcpyHostToDevice \n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk(\n",
        "        cudaMemcpy( mat_host.data, mat_device.data,  \n",
        "                   fmatrix_size(mat_device), \n",
        "                   cudaMemcpyDeviceToHost \n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_copy_to_host(fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_device);\n",
        "    fmatrix mat_host = fmatrix_create_on_host(mat_device.rows, mat_device.cols);\n",
        "    fmatrix_data_to_host(mat_host,mat_device);\n",
        "    return mat_host;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_copy_to_device(fmatrix mat_host) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix mat_device = fmatrix_create_on_device(mat_host.rows, mat_host.cols);\n",
        "    fmatrix_data_to_device(mat_host,mat_device);\n",
        "    return mat_device;\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_host(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);  \n",
        "  free(mat->data);\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_device(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);  \n",
        "  gpuErrchk(cudaFree(mat->data));\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_subcolumns(fmatrix M, int a, int b) {\n",
        "    fmatrix_assert(M);  \n",
        "    fmatrix A = { \n",
        "        .data = &getfm(M,0,a),  \n",
        "        .cols = b-a,\n",
        "        .rows = M.rows \n",
        "    };\n",
        "    fmatrix_assert(A);  \n",
        "    return A;\n",
        "}\n",
        "\n",
        "\n",
        "__host__\n",
        "__device__\n",
        "void fmatrix_print(fmatrix mat, int nb){\n",
        "    if (nb<0 || nb > mat.rows) {\n",
        "        nb = mat.rows;\n",
        "    }\n",
        "    printf(\"[\\n\");\n",
        "    for (int i = 0 ; i < nb; i++){\n",
        "      for (int j = 0 ; j<mat.cols; j++){\n",
        "        printf(\"%f\", getfm(mat,i,j));\n",
        "        if (j+1<mat.cols) {\n",
        "          printf(\",\\t\");\n",
        "        }\n",
        "      }\n",
        "      if (i+1<nb) {\n",
        "        printf(\";\\n\");\n",
        "      }\n",
        "    }\n",
        "    if (nb < mat.rows) {\n",
        "      printf(\"\\n...\\n\");\n",
        "    }\n",
        "  printf(\"\\n]\\n\");\n",
        "}\n",
        "\n",
        "void fmatrix_device_print(fmatrix mat, int nb){\n",
        "   // allocate copy\n",
        "   fmatrix tmp = fmatrix_copy_to_host(mat);\n",
        "   fmatrix_print(tmp,nb);\n",
        "   fmatrix_free_on_host(&tmp);\n",
        "}\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fmatrix.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsKLTK8ELOdN"
      },
      "source": [
        "## Data I/O"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD7rmOBmWfsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e8003b3-c8ec-4198-e771-d31b7ee5f506"
      },
      "source": [
        "%%writefile read_csv.cuh\n",
        "#include <cuda_runtime.h>\n",
        "#ifndef read_csv_H\n",
        "#define read_csv_H\n",
        "\n",
        "void read_csv(const char* filename, float* data_array,int nbrow,int nbcol);\n",
        "\n",
        "#endif"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing read_csv.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeedFsZ_WQx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf978d0-2bcc-44d5-f201-8557a01dc1b3"
      },
      "source": [
        "%%writefile read_csv.cu\n",
        "\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <fstream>\n",
        "\n",
        "#include \"read_csv.cuh\"\n",
        "#include \"cuda_stuff.cuh\" // for matrix indexing\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Functions for reading the dataset from a file\n",
        "/////////////////////////////////////////////////////////\n",
        "\n",
        "/* Read a csv file with a given number of rows and columns */\n",
        "void read_csv(const char* filename, float* data_array,int nbrow,int nbcol) {\n",
        "  string row_as_string;\n",
        "  string value;\n",
        "  double ioTemp;\n",
        "  ifstream infile;\n",
        "  infile.open(filename, ifstream::in);\n",
        "  int row_count = 0;\n",
        "\tif (infile.is_open())\n",
        "  {\n",
        "      // read the headers (and discard)\n",
        "\t\t\tgetline(infile, row_as_string, '\\n');\n",
        "      cout << \"headers: \" << row_as_string << \"!\" << std::endl;\n",
        "      for(int i = 0; i < nbrow; i++){\n",
        "  \t\t\tgetline(infile, row_as_string, '\\n');\n",
        "        // cout << \"read line \" << row_as_string << \"!\" << std::endl;\n",
        "\t\t\t\tistringstream line_stream(row_as_string);\n",
        "\t\t\t  for(int j = 0; j < nbcol; j++){\n",
        "          getline(line_stream, value, ',');\n",
        "\t\t\t\t\tioTemp = strtod(value.c_str(), NULL); \n",
        "          // cout << \"(\"<<i<<\",\"<<j<<\") = \"<< ioTemp << std::endl;\n",
        "\n",
        "\t\t\t\t\tdata_array[IDX2C(i,j,nbrow)] = ioTemp;\n",
        "\n",
        "\t\t\t\t}\n",
        "        ++row_count;\n",
        "\t\t\t}\n",
        "\t\tinfile.close();\n",
        "    cout << \"Read \" << row_count << \" rows.\" << std::endl;\n",
        "\t}\n",
        "\telse cout << \"Cannot open file.\" << endl;\n",
        "}\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing read_csv.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex8ilQdYYroU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27308e89-5b07-45a7-aad7-29b91336a8c2"
      },
      "source": [
        "%%writefile preprocess_data.cuh\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#ifndef preprocess_data_H\n",
        "#define preprocess_data_H\n",
        "\n",
        "void get_inputs_and_labels(float* data_array, float** input_array, float** label_array, int nbrows, int nbcols, int nb_inputs, int nb_labels );\n",
        "\n",
        "#endif"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing preprocess_data.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaeUdw_KYaCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23fd3c0-a0ea-4141-f5d6-b50df3f7c498"
      },
      "source": [
        "%%writefile preprocess_data.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <fstream>\n",
        "\n",
        "/*Matrix multiplication functions and other auxiliary functions*/\n",
        "#include \"preprocess_data.cuh\"\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major, \n",
        "   ld = number of rows \n",
        "   Example of use: a[IDX2C(0, 1, 50)] */\n",
        "#define IDX2C(i,j,ld) (((j)*(ld))+(i))\n",
        "\n",
        "//Number of thread per block\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "/* Constants for housing data set */\n",
        "#define data_columns  (9)\n",
        "#define above_threshold (265000.0)\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Number of rows in arrays to print for debugging\n",
        "/////////////////////////////////////////////////////////\n",
        "#define print_rows (10)\n",
        "/////////////////////////////////////////////////////////\n",
        "// Functions for preprocessing the data set\n",
        "/////////////////////////////////////////////////////////\n",
        "\n",
        "/* Split data into inputs and labels. Allocated memory for inputs and labels.\n",
        "   Since cuBLAS is column major, each input is in a column.\n",
        "   We also add 1.0 as first element to each input vector.\n",
        "*/\n",
        "void get_inputs_and_labels(float* data_array, float** input_array, float** label_array, int nbrows, int nbcols, int nb_inputs, int nb_labels ) {\n",
        "    // The inputs are the first nbrows-1 columns.\n",
        "    // The labels are the last column (index nbrows-1), booleanized\n",
        "    // by the condition >= above_threshold\n",
        "    *input_array = (float *)malloc(nbrows * nb_inputs * sizeof(float));    \n",
        "    *label_array = (float *)malloc(nbrows * nb_labels * sizeof(float));    \n",
        "    //cout << &input_array << \" and \"<< &label_array << \" data \" << data_array << std::endl;\n",
        "    cout << \"Allocated memory for inputs: \" << nbrows << \" rows, \"<< nb_inputs << \" columns.\" << std::endl;\n",
        "    cout << \"Allocated memory for labels: \" << nbrows << \" rows, \"<< nb_labels << \" columns.\" << std::endl;\n",
        "\n",
        "    // Copy the data to X\n",
        "    for(int i = 0; i < nbrows; i++){\n",
        "      // Set the first element of each x to 1  \n",
        "      (*input_array)[IDX2C(0,i,nb_inputs)] = 1.0;\n",
        "      // Copy the rest of x\n",
        "\t\t\tfor(int j = 1; j < nb_inputs; j++){\n",
        "\t\t\t\t(*input_array)[IDX2C(j,i,nb_inputs)] = data_array[IDX2C(i,j-1,nbrows)];\n",
        "\t\t\t}\n",
        "      float median_house_value = data_array[IDX2C(i,nbcols-1,nbrows)];\n",
        "      (*label_array)[IDX2C(0,i,nb_labels)] = 0.0;\n",
        "      (*label_array)[IDX2C(1,i,nb_labels)] = 0.0;\n",
        "      if (median_house_value >= above_threshold) {\n",
        "        (*label_array)[IDX2C(0,i,nb_labels)] = 1.0;\n",
        "      } else {\n",
        "        (*label_array)[IDX2C(1,i,nb_labels)] = 1.0;        \n",
        "      }\n",
        "\t\t}    \n",
        "    \n",
        "    // Show some entries for double checking\n",
        "    cout << \"Inputs (first \"<<print_rows<<\"):\" << std::endl;\n",
        "\t  for(int j = 0; j < nb_inputs; j++){\n",
        "      for(int i = 0; i < nbrows && i < print_rows; i++){\n",
        "\t\t\t\tcout << (*input_array)[IDX2C(j,i,nb_inputs)] << \"\\t\";\n",
        "\t\t\t}\n",
        "      cout << \"\\n\";\n",
        "\t\t}\n",
        "    cout << \"Labels (first \"<<print_rows<<\"):\" << std::endl;\n",
        "    for(int j = 0; j < nb_labels; j++){\n",
        "      for(int i = 0; i < nbrows && i < print_rows; i++){\n",
        "\t\t\t\tcout << (*label_array)[IDX2C(j,i,nb_labels)] << \"\\t\";\n",
        "\t\t\t}\n",
        "      cout << \"\\n\";\n",
        "\t\t}\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing preprocess_data.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHy3EAid05oA"
      },
      "source": [],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code That You Write"
      ],
      "metadata": {
        "id": "rR-9WFucUWLC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pWS3hlAecOc"
      },
      "source": [
        "## Classifier Math"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK_tmB-xbZKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106a390d-4e01-42db-df0f-a0e17c39a4bb"
      },
      "source": [
        "%%writefile classifier_math.cuh\n",
        "#ifndef classifier_math_H\n",
        "#define classifier_math_H\n",
        "\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "/** Returns a random float between min and max (including). */\n",
        "float float_rand( float min, float max );\n",
        "\n",
        "/** Initialize W with Xavier's method,\n",
        " *  scaled by a. */\n",
        "void xavier_weight_init(float a, fmatrix W);\n",
        "\n",
        "/** Compute the softmax for each column of Z and store in P **/\n",
        "void softmax_col(fmatrix P,fmatrix Z); \n",
        "\n",
        "///////////////////////////////////\n",
        "// TO BE COMPLETED\n",
        "// ... add your matrix math here\n",
        "///////////////////////////////////\n",
        "\n",
        "void fmatrix_mult(fmatrix A, float f, fmatrix B, fmatrix C);\n",
        "\n",
        "void fmatrix_transpose(fmatrix Z, fmatrix Z_T);\n",
        "\n",
        "void fmatrix_add(fmatrix P,float a,fmatrix Y);\n",
        "\n",
        "void sigma_mu(fmatrix A, fmatrix mu, fmatrix sigma);\n",
        "\n",
        "void normalization(fmatrix A, fmatrix mu, fmatrix sigma);\n",
        "\n",
        "#endif"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing classifier_math.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgXwgv6Bbo-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910043ab-2604-4fc2-e98d-3897b7535067"
      },
      "source": [
        "%%writefile classifier_math.cu\n",
        "#include \"classifier_math.cuh\"\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include <math.h>\n",
        "#include <assert.h>\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Auxiliary function\n",
        "/////////////////////////////////////////////////////////\n",
        "// generate random numbers in interval [min,max]\n",
        "float float_rand( float min, float max )\n",
        "{\n",
        "    float scale = rand() / (float) RAND_MAX; /* [0, 1.0] */\n",
        "    return min + scale * ( max - min );      /* [min, max] */\n",
        "}\n",
        "\n",
        "void xavier_weight_init(float a, fmatrix W){\n",
        "    for (int j = 0; j < W.rows  ; ++j) {\n",
        "      for (int i = 0; i < W.cols  ; ++i) {\n",
        "          getfm(W,j,i) = a * (1.0/sqrt(W.cols+W.rows)) * float_rand(-1.0,1.0);\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void stable_softmax_kernel(fmatrix P, fmatrix Z) {\n",
        "  \n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / Z.rows;\n",
        "    int i = idx % Z.rows;\n",
        "    if (i < Z.rows && j < Z.cols ){\n",
        "      //getfm(P,i,j) = getfm(Z,i,j);\n",
        "      float sum = 0;\n",
        "      float max = getfm(Z,0,j);\n",
        "      for (int k=0; k<Z.rows; k++){ \n",
        "          if(getfm(Z,k,j) > max){\n",
        "            max = getfm(Z,k,j);\n",
        "            }\n",
        "      }\n",
        "        for (int k=0; k<Z.rows; k++){\n",
        "          sum += expf(getfm(Z,k,j) - max);\n",
        "      }\n",
        "      getfm(P,i,j) = expf(getfm(Z,i,j) - max) / sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void fmatrix_transpose_kernel(fmatrix Z, fmatrix Z_T) {\n",
        "  \n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / Z.rows;\n",
        "    int i = idx % Z.rows;\n",
        "\n",
        "    getfm(Z_T,j,i) = getfm(Z,i,j);\n",
        "\n",
        "    //int j = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    //int i = blockIdx.y*blockDim.y+threadIdx.y;\n",
        "    //if (j < Z.rows && i < Z.cols ) \n",
        "    /*for(int i=0; i<Z.cols; i++){\n",
        "      for(int j=0; j<Z.rows; j++){\n",
        "          getfm(Z_T,j,i) = getfm(Z,i,j);\n",
        "      }\n",
        "    }*/\n",
        "}\n",
        "\n",
        "__global__ \n",
        "void fmatrix_add_kernel(fmatrix P,float a,fmatrix Y) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / P.rows;\n",
        "    int i = idx % P.rows;\n",
        "    if (i < P.rows && j < P.cols ){\n",
        "        getfm(P,i,j) += a*getfm(Y,i,j);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void fmatrix_multiplication_kernel(fmatrix A, float f, fmatrix B, fmatrix C) {\n",
        "    // Each thread multiplies one row of B with one column of C\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / A.rows;\n",
        "    int i = idx % A.rows;\n",
        "    if (i < A.rows && j < A.cols ){\n",
        "        getfm(A,i,j) = 0.0;\n",
        "        for (int k = 0; k < B.cols; ++k) {\n",
        "          getfm(A,i,j) += f*getfm(B,i,k)*getfm(C,k,j); \n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_transpose(fmatrix Z, fmatrix Z_T) {\n",
        "\n",
        "    assert(Z.rows == Z_T.cols);\n",
        "    assert(Z.cols == Z_T.rows);\n",
        "    \n",
        "    \n",
        "    int threadsPerBlock = fmatrix_elements(Z);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    \n",
        "    fmatrix_transpose_kernel<<< blocksPerGrid, threadsPerBlock >>>(Z, Z_T); // 1 thread pour la transposition\n",
        "\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    device_synchronize();\n",
        "\n",
        "  }\n",
        "\n",
        "\n",
        "void softmax_col(fmatrix P,fmatrix Z) {\n",
        "    assert(P.cols==Z.cols);\n",
        "    assert(P.rows==Z.rows);\n",
        "    \n",
        "    ///////////////////////////////////\n",
        "    // TO BE COMPLETED\n",
        "    // ... compute the softmax here ...\n",
        "    ///////////////////////////////////\n",
        "\n",
        "    \n",
        "    int threadsPerBlock = fmatrix_elements(Z);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    stable_softmax_kernel<<< blocksPerGrid, threadsPerBlock >>>(P,Z);\n",
        "    \n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    device_synchronize();\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "/* Compute A = f*B^T*C */\n",
        "void fmatrix_mult(fmatrix A, float f, fmatrix B, fmatrix C) {\n",
        "    // First let's check for errors in the argument M.\n",
        "    // This can help a LOT when debugging.\n",
        "    // A,B,C need to have nonzero pointers etc.\n",
        "    fmatrix_assert(A);\n",
        "    fmatrix_assert(B);\n",
        "    fmatrix_assert(C);\n",
        "    assert(A.rows == B.rows);\n",
        "    assert(A.cols == C.cols);\n",
        "    assert(B.cols == C.rows);\n",
        "    \n",
        "    int threadsPerBlock = fmatrix_elements(A);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_multiplication_kernel<<< blocksPerGrid, threadsPerBlock >>>(A,f,B,C);\n",
        "\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    device_synchronize();\n",
        "\n",
        "  }\n",
        "\n",
        "/** Compute P = P + a*Y */\n",
        "void fmatrix_add(fmatrix P,float a,fmatrix Y) {\n",
        "    fmatrix_assert(P);\n",
        "    fmatrix_assert(Y);\n",
        "    assert(P.rows == Y.rows);\n",
        "    assert(P.cols == Y.cols);\n",
        "    int threadsPerBlock = fmatrix_elements(P);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_add_kernel<<< blocksPerGrid, threadsPerBlock >>>(P,a,Y);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__global__\n",
        "void sigma_mu_kernel(fmatrix X, fmatrix mu, fmatrix sigma ) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / X.rows;\n",
        "    int i = idx % X.rows;\n",
        "    if (i < X.rows && j < X.cols ){\n",
        "      getfm(mu,i,0) = 0;\n",
        "      for (int k=0; k<X.cols; k++){\n",
        "          getfm(mu,i,0) += getfm(X,i,k);\n",
        "      }\n",
        "      getfm(mu,i,0) = getfm(mu,i,0) / (float) X.cols;\n",
        "\n",
        "      getfm(sigma,i,0) = 0;\n",
        "      for (int k=0; k<X.cols; k++){\n",
        "          getfm(sigma,i,0) += powf((getfm(X,i,k) - getfm(mu,i,0)), 2.0);\n",
        "      }\n",
        "      getfm(sigma,i,0) =  sqrtf(getfm(sigma,i,0) / (float) X.cols);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void normalization_kernel(fmatrix X, fmatrix mu, fmatrix sigma) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / X.rows;\n",
        "    int i = idx % X.rows;\n",
        "\n",
        "    if (i < X.rows && j < X.cols ){\n",
        "      if (getfm(sigma,i,0) != 0){\n",
        "      getfm(X,i,j) = (getfm(X,i,j) - getfm(mu,i,0))/getfm(sigma,i,0) ;\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "/* Compute sigma mu (A) */\n",
        "void sigma_mu(fmatrix A, fmatrix mu, fmatrix sigma) {\n",
        "    \n",
        "    // take one thread per element, and distribute\n",
        "    // over as many blocks as necessary given\n",
        "    // the hardware limit on the number of threads per block\n",
        "\n",
        "    int threadsPerBlock = fmatrix_elements(A);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    sigma_mu_kernel<<< blocksPerGrid, threadsPerBlock >>>(A, mu, sigma);\n",
        "    // check for errors\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    // wait for the kernel to finish\n",
        "    device_synchronize();\n",
        "}\n",
        "\n",
        "/* Compute normalization(A) */\n",
        "void normalization(fmatrix A, fmatrix mu, fmatrix sigma) {\n",
        "    \n",
        "    // take one thread per element, and distribute\n",
        "    // over as many blocks as necessary given\n",
        "    // the hardware limit on the number of threads per block\n",
        "\n",
        "    int threadsPerBlock = fmatrix_elements(A);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    normalization_kernel<<< blocksPerGrid, threadsPerBlock >>>(A, mu, sigma);\n",
        "    // check for errors\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    // wait for the kernel to finish\n",
        "    device_synchronize();\n",
        "}\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing classifier_math.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6G22rDKR3rv"
      },
      "source": [
        "## Evaluating Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd7Vmzo72hpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9281ec69-677a-4124-ae38-72db36619e46"
      },
      "source": [
        "%%writefile evaluate_accuracy.cuh\n",
        "\n",
        "/** Evaluate the accuracy of a linear classifier with D x M weight\n",
        " *  matrix W, using D x N input data X and M x N output labels Y.\n",
        " *  Z is a temporary matrix with dimensions M x N,\n",
        " *  which must be previously allocated.\n",
        " */\n",
        "float evaluate_accuracy(fmatrix d_W,fmatrix d_X,fmatrix d_Y,fmatrix d_Z);\n",
        "\n",
        "/** Compute the logloss given M x N matrices of \n",
        " *  probabilities P and output labels Y\n",
        " *  and stores it in J.\n",
        " *  J is a matrix with dimensions 1 x 1,\n",
        " *  which must be previously allocated.\n",
        " *  logloss = sum_j -Y(j,k)*log(P(j,k))\n",
        " */\n",
        "void evaluate_logloss(fmatrix d_P,fmatrix d_Y,fmatrix d_J);"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing evaluate_accuracy.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0Z-9B4a2dwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc0f53a-ef4c-4e73-c61a-a3f4d2b77553"
      },
      "source": [
        "%%writefile evaluate_accuracy.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"classifier_math.cuh\"\n",
        "#include <assert.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "\n",
        "__global__ \n",
        "void evaluate_accuracy_kernel(fmatrix d_Y,fmatrix d_Z,int* count) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    if (idx < d_Z.cols){\n",
        "        float z_max = getfm(d_Z,0,idx);\n",
        "        int i_max = 0;\n",
        "        for (int i = 1; i < d_Z.rows; ++i) {\n",
        "          if (getfm(d_Z,i,idx)>z_max) {\n",
        "                z_max = getfm(d_Z,i,idx);\n",
        "                i_max = i;\n",
        "          }\n",
        "        }\n",
        "      if (getfm(d_Y,i_max,idx)>=0.5f) {\n",
        "          atomicAdd(count,1);\n",
        "      }\n",
        "    }    \n",
        "}\n",
        "\n",
        "float evaluate_accuracy(fmatrix d_W,fmatrix d_X,fmatrix d_Y,fmatrix d_Z) {\n",
        "    assert(d_Y.cols == d_Z.cols);\n",
        "    assert(d_Y.rows == d_Z.rows);\n",
        "\n",
        "  //////////////////////////////////////////\n",
        "  // 1. compute Z = W^T X\n",
        "  // --> each column of Z corresponds to one input\n",
        "  //////////////////////////////////////////\n",
        "  /*********************************\n",
        "  / TO BE COMPLETED\n",
        "  / ... compute Z = W^T X here ...\n",
        "  **********************************/\n",
        "  /* Compute d_Z = f*d_W^T*d_X */\n",
        "\n",
        "  fmatrix d_W_T = fmatrix_create_on_device(d_W.cols,d_W.rows);\n",
        "  fmatrix_transpose(d_W, d_W_T);\n",
        "  fmatrix_mult(d_Z, 1.0, d_W_T, d_X);\n",
        "  fmatrix_free_on_device(&d_W_T);\n",
        "\n",
        "\n",
        "  //////////////////////////////////////////\n",
        "  // 2. For each column z of Z, \n",
        "  // find argmax_k z_k\n",
        "  //////////////////////////////////////////\n",
        "\n",
        "\n",
        "  int true_class = 0;\n",
        "\n",
        "  int* d_count = 0;\n",
        "  gpuErrchk(cudaMalloc((void **)&d_count, sizeof(int)));\n",
        "  gpuErrchk( \n",
        "        cudaMemcpy( d_count, &true_class, sizeof(int), cudaMemcpyHostToDevice )\n",
        "  );\n",
        "\n",
        "    int threadsPerBlock = d_Z.cols;\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    evaluate_accuracy_kernel<<< blocksPerGrid, threadsPerBlock >>>(d_Y,d_Z,d_count);\n",
        "    device_synchronize();\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "\n",
        "  gpuErrchk(\n",
        "          cudaMemcpy(&true_class, d_count, sizeof(int), cudaMemcpyDeviceToHost )\n",
        "  );\n",
        "\n",
        "  //printf(\"Correct results: %d out of %d\\n\",true_class,nb_tested);\n",
        "  //printf(\"Accuracy: %f\\n\",(float)true_class/(float)nb_tested);\n",
        "  return (float)true_class/(float)d_Z.cols;\n",
        "}\n",
        "\n",
        "__global__\n",
        "void evaluate_logloss_kernel(fmatrix d_P,fmatrix d_Y, fmatrix d_J) {\n",
        "  \n",
        "    getfm(d_J,0,0) = 0.0;\n",
        "    float temp = 0.0;\n",
        "    for(int k=0; k<d_Y.cols ; k++)\n",
        "    {\n",
        "      for(int j=0; j<d_Y.rows; j++){\n",
        "        if(getfm(d_Y,j,k) > 0.0 && getfm(d_P,j,k) > 0.0){\n",
        "          temp = -getfm(d_Y,j,k)*logf(getfm(d_P,j,k));\n",
        "          getfm(d_J,0,0) += temp;\n",
        "        }   \n",
        "      }\n",
        "    }\n",
        "    \n",
        "    getfm(d_J,0,0) *= 1.0 / d_Y.cols; \n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "void evaluate_logloss(fmatrix d_P,fmatrix d_Y,fmatrix d_J) {\n",
        "    assert(d_Y.cols == d_P.cols);\n",
        "    assert(d_Y.rows == d_P.rows);\n",
        "    fmatrix_assert(d_Y);\n",
        "    fmatrix_assert(d_P);\n",
        "\n",
        "\n",
        "    //float J = 0.0;\n",
        "\n",
        "    ///////////////////////////////////\n",
        "    // TO BE COMPLETED\n",
        "    // ... compute the logloss here ...\n",
        "    /////////////////////////////////// \n",
        "\n",
        "    evaluate_logloss_kernel<<< 1, 1 >>>(d_P, d_Y, d_J);\n",
        "\n",
        "    device_synchronize();\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "\n",
        "    //return J;\n",
        "}\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting evaluate_accuracy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AwZh8WULi_F"
      },
      "source": [
        "## Linear Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWO5p1NeHE9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd0c518d-b51d-4959-80d8-2028d7d04101"
      },
      "source": [
        "%%writefile linear_classification.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "#include <fstream>\n",
        "\n",
        "/*Matrix multiplication functions and other auxiliary functions*/\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"read_csv.cuh\"\n",
        "#include \"preprocess_data.cuh\"\n",
        "#include \"classifier_math.cuh\"\n",
        "#include \"evaluate_accuracy.cuh\"\n",
        "/* Includes, cuda */\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "//Number of thread per block\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "/* Constants for housing data set */\n",
        "#define data_columns  (9)\n",
        "#define above_threshold (265000.0)\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Number of rows in arrays to print for debugging\n",
        "/////////////////////////////////////////////////////////\n",
        "#define print_rows (10)\n",
        "\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "/////////////////////////////////////////////////////////\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Parameters for the data set\n",
        "    /////////////////////////////////////////////////////////\n",
        "    size_t N_train = 12000; //12000; // points for training (Google: 12000)\n",
        "    size_t N_test = 5000; // 5000; // points for validation (Google: 5000)\n",
        "    size_t N = N_train;\n",
        "    size_t Nall = N_train+N_test;\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Reading the data set\n",
        "    /////////////////////////////////////////////////////////\n",
        "    fmatrix alldata = fmatrix_create_on_host(Nall,data_columns);\n",
        "    read_csv(\"sample_data/california_housing_train.csv\",alldata.data,Nall,data_columns);\n",
        "    //fmatrix_print(alldata);\n",
        "\n",
        "    size_t D = data_columns-1+1; // remove output column, add column with const. 1.0\n",
        "    size_t M = 2; // number of labels (one-hot encoding)\n",
        "    fmatrix Xall = fmatrix_create_on_host(D,Nall);\n",
        "    fmatrix Yall = fmatrix_create_on_host(M,Nall);\n",
        "    get_inputs_and_labels(alldata.data,&Xall.data,&Yall.data,Nall,data_columns,D,M);\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Inputs and labels are now available in X and Y.\n",
        "    // Each input is a column in X; X is of dimension D x N\n",
        "    // each label is a column in Y; Y is of dimension M x N\n",
        "    /////////////////////////////////////////////////////////\n",
        "     \n",
        "    // Logfile\n",
        "    FILE* fp = fopen(\"log.txt\", \"w\");\n",
        "    \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Parameters for Stochastic Gradient Descent\n",
        "    /////////////////////////////////////////////////////////\n",
        "    int nb_iter = 16;           // defeault: 10;\n",
        "    int periods = nb_iter;      // reporting period\n",
        "    int batch_size = 400;    // defeault: N;\n",
        "    float learning_rate = 1e-3; // default: 1e-7\n",
        " \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Memory Allocation and Initialization\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // take X,Y to be the first N columns of all data\n",
        "    fmatrix h_X = fmatrix_subcolumns(Xall,0,N);\n",
        "    fmatrix h_Y = fmatrix_subcolumns(Yall,0,N);\n",
        "    fmatrix h_Xtest = fmatrix_subcolumns(Xall,N,Nall);\n",
        "    fmatrix h_Ytest = fmatrix_subcolumns(Yall,N,Nall);\n",
        "    fmatrix h_W = fmatrix_create_on_host(D,M);\n",
        "    fmatrix h_J = fmatrix_create_on_host(1,1);\n",
        "\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Initializing Weight Matrix \n",
        "    // its dimension is D x M\n",
        "    /////////////////////////////////////////////////////////\n",
        "    xavier_weight_init(1.0,h_W);\n",
        " \n",
        "    //////////////////////////////\n",
        "    // Copy data to device      //\n",
        "    //////////////////////////////\n",
        "    fmatrix d_X = fmatrix_copy_to_device(h_X);\n",
        "    fmatrix d_Y = fmatrix_copy_to_device(h_Y);\n",
        "    fmatrix d_Xtest = fmatrix_copy_to_device(h_Xtest);\n",
        "    fmatrix d_Ytest = fmatrix_copy_to_device(h_Ytest);\n",
        "    fmatrix d_W = fmatrix_copy_to_device(h_W);\n",
        "    fmatrix d_J = fmatrix_copy_to_device(h_J);\n",
        " \n",
        "    /////////////////////////////////////////\n",
        "    // Create auxiliary matrices on device //\n",
        "    /////////////////////////////////////////\n",
        "    fmatrix d_Z = fmatrix_create_on_device(M,batch_size);\n",
        "    fmatrix d_P = fmatrix_create_on_device(M,batch_size);\n",
        "    fmatrix d_G = fmatrix_create_on_device(D,M);\n",
        "    // auxiliary matrix for computing Z=W^T X on test data\n",
        "    fmatrix d_Ztest = fmatrix_create_on_device(M,d_Xtest.cols);\n",
        "\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Batch Gradient Descent\n",
        "    /////////////////////////////////////////////////////////\n",
        "    //fmatrix_device_print(d_X);\n",
        "    //fmatrix_device_print(d_W);\n",
        "\n",
        "    fmatrix mu = fmatrix_create_on_device(d_X.rows,1);\n",
        "    fmatrix sigma = fmatrix_create_on_device(d_X.rows,1);\n",
        "    sigma_mu(d_X, mu, sigma);\n",
        "    normalization(d_X, mu, sigma);\n",
        "\n",
        "    fmatrix mu_test = fmatrix_create_on_device(d_X.rows,1);\n",
        "    fmatrix sigma_test = fmatrix_create_on_device(d_X.rows,1);\n",
        "    sigma_mu(d_Xtest, mu_test, sigma_test);\n",
        "    normalization(d_Xtest, mu_test, sigma_test);\n",
        " \n",
        "     /* Evaluate the accuracy */\n",
        "    float accuracy = 0;\n",
        "    accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "    printf(\"initial accuracy: %f\\n\",accuracy);\n",
        "\n",
        "    float J = 0;\n",
        " \n",
        "    clock_t tstart_total, tend;\n",
        "    tstart_total = clock();\n",
        " \n",
        "    //int batch_pointer = 0;\n",
        "    \n",
        "    for (int i=0; i<nb_iter; ++i){\n",
        "\n",
        "        for(int batch_pointer = 0; batch_pointer < (N / batch_size) + 1; batch_pointer++) {\n",
        "\n",
        "                int min = batch_pointer + batch_size;\n",
        "                if (min > N) min = N;\n",
        "\n",
        "                fmatrix dX_batch = fmatrix_subcolumns(d_X, batch_pointer, min);\n",
        "                fmatrix dY_batch = fmatrix_subcolumns(d_Y, batch_pointer, min);\n",
        "        \n",
        "                ////////////////////////////////\n",
        "                // compute Z = W^T X\n",
        "                // --> each column z of Z corresponds to one column x of X\n",
        "                ////////////////////////////////\n",
        "                \n",
        "                ///////////////////////////////////\n",
        "                // TO BE COMPLETED\n",
        "                ///////////////////////////////////\n",
        "\n",
        "\n",
        "                fmatrix d_W_T = fmatrix_create_on_device(d_W.cols,d_W.rows);\n",
        "                fmatrix_transpose(d_W, d_W_T);\n",
        "\n",
        "                fmatrix_mult(d_Z, 1.0, d_W_T, dX_batch);\n",
        "                fmatrix_free_on_device(&d_W_T);\n",
        "\n",
        "                ////////////////////////////////\n",
        "                // For each column z of Z, compute activation p(z);\n",
        "                // then update W\n",
        "                ////////////////////////////////\n",
        "\n",
        "                // compute softmax per column of Z and store in z\n",
        "\n",
        "                softmax_col(d_P,d_Z);\n",
        "\n",
        "\n",
        "\n",
        "                ///////////////////////////////////\n",
        "                // TO BE COMPLETED\n",
        "                ///////////////////////////////////\n",
        "                  \n",
        "                // Q:=P-Y\n",
        "                // compute gradient G = XQ^T\n",
        "                // ... possibly work with G here ...\n",
        "                // update weights W = W - learning_rate*G\n",
        "\n",
        "                ///////////////////////////////////\n",
        "                // TO BE COMPLETED\n",
        "                ///////////////////////////////////\n",
        "                \n",
        "                fmatrix h_Q = fmatrix_create_on_host(M,batch_size);\n",
        "                fmatrix d_Q = fmatrix_create_on_device(M,batch_size);\n",
        "                fmatrix d_Q_T = fmatrix_create_on_device(batch_size,M);\n",
        "\n",
        "                for(int l=0; l<M; l++){\n",
        "                  for(int j=0; j<batch_size; j++){\n",
        "                    getfm(h_Q,l,j) = 0.0;\n",
        "                  }\n",
        "                }\n",
        "\n",
        "                fmatrix_data_to_device(h_Q, d_Q);\n",
        "                \n",
        "                fmatrix_add(d_Q, 1.0, d_P);\n",
        "                fmatrix_add(d_Q, -1.0, dY_batch);\n",
        "\n",
        "                fmatrix_transpose(d_Q, d_Q_T);\n",
        "                fmatrix_mult(d_G, 1.0, dX_batch, d_Q_T);\n",
        "\n",
        "                fmatrix_add(d_W, -learning_rate , d_G);\n",
        "\n",
        "                \n",
        "                ///////////////////////////////////\n",
        "                // TO BE COMPLETED\n",
        "                ///////////////////////////////////\n",
        "\n",
        "                // evaluate logloss (for reporting only)\n",
        "                \n",
        "                evaluate_logloss(d_P, dY_batch, d_J);\n",
        "                fmatrix_data_to_host(h_J,d_J);\n",
        "                J = getfm(h_J,0,0);\n",
        "\n",
        "        }\n",
        "\n",
        "                \n",
        "        //printf(\"W:\\n\");fmatrix_device_print(d_W);\n",
        "        \n",
        "        ////////////////////////////////\n",
        "        // For reporting, compute logloss and accuracy\n",
        "        ////////////////////////////////\n",
        "        if (i%(nb_iter/periods)==0) {\n",
        "          float accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "          printf(\"iter: %d, logloss: %f, accuracy: %f\\n\",i,J, accuracy);\n",
        "          fprintf(fp, \"%f,%f\\n\", J, accuracy);\n",
        "        }\n",
        "       \n",
        "    }\n",
        "    tend = clock();\n",
        "    float duration = ((float)(tend-tstart_total))/CLOCKS_PER_SEC;\n",
        "    printf(\"Duration (s): %f\\n\",duration);\n",
        "    /* Evaluate the accuracy */\n",
        "    accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "    printf(\"final accuracy: %f\\n\",accuracy);\n",
        "\n",
        "    printf(\"final weights: \\n\");\n",
        "    fmatrix_device_print(d_W);\n",
        "\n",
        "    /* Memory clean up */\n",
        "    /** No need to free h_X, h_Y, h_Xtest, h_Ytest since \n",
        "     *  they all point to Xall \n",
        "     */\n",
        "    fmatrix_free_on_host(&h_W);\n",
        "    fmatrix_free_on_host(&Xall);\n",
        "    fmatrix_free_on_host(&Yall);\n",
        "\n",
        "    fmatrix_free_on_device(&d_X);\n",
        "    fmatrix_free_on_device(&d_Y);\n",
        "    fmatrix_free_on_device(&d_Xtest);\n",
        "    fmatrix_free_on_device(&d_Ytest);\n",
        "    fmatrix_free_on_device(&d_W);\n",
        "    fmatrix_free_on_device(&d_Z);\n",
        "    fmatrix_free_on_device(&d_J);\n",
        " \n",
        "    // Close log file\n",
        "    fclose(fp);\n",
        "}"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting linear_classification.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrATC8s9LsDw"
      },
      "source": [
        "# Compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z52xd0NMRKXb"
      },
      "source": [
        "!nvcc -arch=sm_35 -Wno-deprecated-gpu-targets -I /content/cuda-samples/Common/ -L/usr/local/cuda/include -lcublas -lcusolver linear_classification.cu read_csv.cu preprocess_data.cu evaluate_accuracy.cu fmatrix.cu classifier_math.cu cuda_stuff.cu"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZVqTfXcLvPr"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV_vFkIT7fV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240b11ab-d998-445b-cf4e-b27850ad45d4"
      },
      "source": [
        "%%time\n",
        "!./a.out "
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "headers: \"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"!\n",
            "Read 17000 rows.\n",
            "Allocated memory for inputs: 17000 rows, 9 columns.\n",
            "Allocated memory for labels: 17000 rows, 2 columns.\n",
            "Inputs (first 10):\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "-114.31\t-114.47\t-114.56\t-114.57\t-114.57\t-114.58\t-114.58\t-114.59\t-114.59\t-114.6\t\n",
            "34.19\t34.4\t33.69\t33.64\t33.57\t33.63\t33.61\t34.83\t33.61\t34.83\t\n",
            "15\t19\t17\t14\t20\t29\t25\t41\t34\t46\t\n",
            "5612\t7650\t720\t1501\t1454\t1387\t2907\t812\t4789\t1497\t\n",
            "1283\t1901\t174\t337\t326\t236\t680\t168\t1175\t309\t\n",
            "1015\t1129\t333\t515\t624\t671\t1841\t375\t3134\t787\t\n",
            "472\t463\t117\t226\t262\t239\t633\t158\t1056\t271\t\n",
            "1.4936\t1.82\t1.6509\t3.1917\t1.925\t3.3438\t2.6768\t1.7083\t2.1782\t2.1908\t\n",
            "Labels (first 10):\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "initial accuracy: 0.297000\n",
            "iter: 0, logloss: 0.111424, accuracy: 0.694000\n",
            "iter: 1, logloss: 0.097210, accuracy: 0.713800\n",
            "iter: 2, logloss: 0.092274, accuracy: 0.726000\n",
            "iter: 3, logloss: 0.089867, accuracy: 0.732800\n",
            "iter: 4, logloss: 0.088481, accuracy: 0.738200\n",
            "iter: 5, logloss: 0.087599, accuracy: 0.740800\n",
            "iter: 6, logloss: 0.086997, accuracy: 0.741800\n",
            "iter: 7, logloss: 0.086564, accuracy: 0.742800\n",
            "iter: 8, logloss: 0.086238, accuracy: 0.743800\n",
            "iter: 9, logloss: 0.085984, accuracy: 0.745200\n",
            "iter: 10, logloss: 0.085779, accuracy: 0.745600\n",
            "iter: 11, logloss: 0.085609, accuracy: 0.746200\n",
            "iter: 12, logloss: 0.085464, accuracy: 0.746600\n",
            "iter: 13, logloss: 0.085338, accuracy: 0.746800\n",
            "iter: 14, logloss: 0.085226, accuracy: 0.746800\n",
            "iter: 15, logloss: 0.085124, accuracy: 0.747000\n",
            "Duration (s): 0.185782\n",
            "final accuracy: 0.747000\n",
            "final weights: \n",
            "[\n",
            "-0.289620,\t0.431072;\n",
            "-0.717040,\t1.067723;\n",
            "0.405762,\t-0.339912;\n",
            "0.308964,\t-0.246580;\n",
            "0.372053,\t-0.473515;\n",
            "-0.270524,\t0.334606;\n",
            "-0.235879,\t0.162422;\n",
            "0.208596,\t0.315083;\n",
            "1.606275,\t-1.393403\n",
            "]\n",
            "CPU times: user 16.4 ms, sys: 7.55 ms, total: 23.9 ms\n",
            "Wall time: 1.22 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxIyxh0ClrIz"
      },
      "source": [
        "Let's plot the logloss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR2kCNEIlpqQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "72cbec04-5f1a-41be-c3b4-a84ca501b459"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "data = pd.read_csv('log.txt',sep=',',header=None)\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(data[0],label=\"logloss\")\n",
        "plt.legend()\n",
        "ax2=ax.twinx()\n",
        "ax2.plot([], [])\n",
        "ax2.plot(data[1],label=\"accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss = 0.0851 \\\n",
        "accuracy = 0.74700 \\\n",
        "hyperparameters : \\\n",
        "learning rate = 1e-3 \\\n",
        "nb_epochs = 16 \\\n",
        "batch_size = 400"
      ],
      "metadata": {
        "id": "UAeW9sxwF_tL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17aUKkNJqTDZ"
      },
      "source": [
        "# Debugging\n",
        "Compile with debugging info on the host (`-g`) and device (`-G`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcfLGo1UrMq9"
      },
      "source": [
        "!nvcc -g -G -I /content/cuda-samples/Common/ -L/usr/local/cuda/include -lcublas -lcusolver linear_classification.cu read_csv.cu preprocess_data.cu evaluate_accuracy.cu fmatrix.cu classifier_math.cu cuda_stuff.cu"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkuaGO10rRm9"
      },
      "source": [
        "Run the debugger cuda-gdb, stopping at the first error that is detected. Shows first the call stack on the GPU, the values of local variables, then the call stack on the host (thread 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ8nAtzGTRgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abcd242a-3279-4e1c-ca93-51bbb5494ce0"
      },
      "source": [
        "! printf \"set cuda memcheck on\\nset cuda api_failures stop\\ncatch throw\\nr\\nbt\\ninfo locals\\nthread 1\\nbt\\n\" > tmp.txt\n",
        "! cat tmp.txt\n",
        "! cuda-gdb -batch -x tmp.txt ./a.out"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "set cuda memcheck on\n",
            "set cuda api_failures stop\n",
            "catch throw\n",
            "r\n",
            "bt\n",
            "info locals\n",
            "thread 1\n",
            "bt\n",
            "Catchpoint 1 (throw)\n",
            "warning: Error disabling address space randomization: Operation not permitted\n",
            "[Thread debugging using libthread_db enabled]\n",
            "Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n",
            "headers: \"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"!\n",
            "Read 17000 rows.\n",
            "Allocated memory for inputs: 17000 rows, 9 columns.\n",
            "Allocated memory for labels: 17000 rows, 2 columns.\n",
            "Inputs (first 10):\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "-114.31\t-114.47\t-114.56\t-114.57\t-114.57\t-114.58\t-114.58\t-114.59\t-114.59\t-114.6\t\n",
            "34.19\t34.4\t33.69\t33.64\t33.57\t33.63\t33.61\t34.83\t33.61\t34.83\t\n",
            "15\t19\t17\t14\t20\t29\t25\t41\t34\t46\t\n",
            "5612\t7650\t720\t1501\t1454\t1387\t2907\t812\t4789\t1497\t\n",
            "1283\t1901\t174\t337\t326\t236\t680\t168\t1175\t309\t\n",
            "1015\t1129\t333\t515\t624\t671\t1841\t375\t3134\t787\t\n",
            "472\t463\t117\t226\t262\t239\t633\t158\t1056\t271\t\n",
            "1.4936\t1.82\t1.6509\t3.1917\t1.925\t3.3438\t2.6768\t1.7083\t2.1782\t2.1908\t\n",
            "Labels (first 10):\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "[Detaching after fork from child process 46745]\n",
            "[New Thread 0x7f516f426000 (LWP 46753)]\n",
            "[New Thread 0x7f516ec25000 (LWP 46754)]\n",
            "initial accuracy: 0.338200\n",
            "\n",
            "Illegal access to address (@global)0x7f514c4c5700 detected.\n",
            "\n",
            "Thread 1 \"a.out\" received signal CUDA_EXCEPTION_1, Lane Illegal Address.\n",
            "[Switching focus to CUDA kernel 2, grid 13, block (1,0,0), thread (192,0,0), device 0, sm 2, warp 4, lane 0]\n",
            "0x000055c4260f1150 in fmatrix_transpose_kernel<<<(2,1,1),(1024,1,1)>>> (Z=..., Z_T=...) at classifier_math.cu:57\n",
            "57\t    getfm(Z_T,j,i) = getfm(Z,i,j);\n",
            "#0  0x000055c4260f1150 in fmatrix_transpose_kernel<<<(2,1,1),(1024,1,1)>>> (Z=..., Z_T=...) at classifier_math.cu:57\n",
            "idx = 1216\n",
            "j = 608\n",
            "i = 0\n",
            "[Switching to thread 1 (Thread 0x7f5176f2c000 (LWP 46740))]\n",
            "#0  0x00007f51758232c0 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#0  0x00007f51758232c0 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#1  0x00007f51756b567f in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#2  0x00007f51756a05c2 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#3  0x00007f51758b8e86 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#4  0x00007f51758ba101 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#5  0x00007f5175797f56 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#6  0x00007f51757a8d62 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#7  0x00007f517592f815 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#8  0x00007f51756b3675 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#9  0x00007f517585fff6 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#10 0x00007f51756c2b80 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#11 0x00007f51758cc4a0 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#12 0x00007f517566528f in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#13 0x00007f517566799f in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#14 0x00007f517570940c in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#15 0x000055c423f9be5b in __cudart803 ()\n",
            "#16 0x000055c423ff6cb8 in cudaLaunchKernel ()\n",
            "#17 0x000055c423f97603 in cudaLaunchKernel<char> (func=0x55c423f96c5a <fmatrix_transpose_kernel(fmatrix, fmatrix)> \"\\363\\017\\036\\372UH\\211\\345H\\203\\354 H\\211\\360I\\211\\370L\\211\\306L\\211\\317H\\211\\307H\\211u\\360H\\211}\\370H\\211U\\340H\\211M\\350H\\215U\\340H\\215E\\360H\\211\\326H\\211\\307\\350g\\376\\377\\377\\220\\311\\303\\363\\017\\036\\372UH\\211\\345H\\203\\354pH\\211}\\250\\363\\017\\021E\\244H\\211u\\230dH\\213\\004%(\", gridDim=..., blockDim=..., args=0x7ffd055f2f30, sharedMem=0, stream=0x0) at /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:211\n",
            "#18 0x000055c423f96c32 in __device_stub__Z24fmatrix_transpose_kernel7fmatrixS_ (__par0=..., __par1=...) at /tmp/tmpxft_0000b5d1_00000000-26_classifier_math.cudafe1.stub.c:1\n",
            "#19 0x000055c423f96c98 in fmatrix_transpose_kernel (__cuda_0=..., __cuda_1=...) at classifier_math.cu:51\n",
            "#20 0x000055c423f96090 in fmatrix_transpose (Z=..., Z_T=...) at classifier_math.cu:106\n",
            "#21 0x000055c423f93af1 in main (argc=1, argv=0x7ffd055f3368) at linear_classification.cu:207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGJ6uVNBVHUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4299e8-aab2-4995-aec9-bdd2fbbd9074"
      },
      "source": [
        "!cuda-memcheck ./a.out "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========= CUDA-MEMCHECK\n",
            "========= This tool is deprecated and will be removed in a future release of the CUDA toolkit\n",
            "========= Please use the compute-sanitizer tool as a drop-in replacement\n",
            "headers: \"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"!\n",
            "Read 6 rows.\n",
            "Allocated memory for inputs: 6 rows, 9 columns.\n",
            "Allocated memory for labels: 6 rows, 2 columns.\n",
            "Inputs (first 10):\n",
            "1\t1\t1\t1\t1\t1\t\n",
            "-114.31\t-114.47\t-114.56\t-114.57\t-114.57\t-114.58\t\n",
            "34.19\t34.4\t33.69\t33.64\t33.57\t33.63\t\n",
            "15\t19\t17\t14\t20\t29\t\n",
            "5612\t7650\t720\t1501\t1454\t1387\t\n",
            "1283\t1901\t174\t337\t326\t236\t\n",
            "1015\t1129\t333\t515\t624\t671\t\n",
            "472\t463\t117\t226\t262\t239\t\n",
            "1.4936\t1.82\t1.6509\t3.1917\t1.925\t3.3438\t\n",
            "Labels (first 10):\n",
            "0\t0\t0\t0\t0\t0\t\n",
            "1\t1\t1\t1\t1\t1\t\n",
            "[\n",
            "0.205141,\t-0.063689;\n",
            "0.170715,\t0.179966;\n",
            "0.248233,\t-0.182383;\n",
            "-0.099364,\t0.161749;\n",
            "-0.134007,\t0.032545;\n",
            "-0.013630,\t0.077712;\n",
            "-0.081538,\t0.008081;\n",
            "0.272705,\t0.250975;\n",
            "0.081837,\t0.131035\n",
            "]\n",
            "initial accuracy: 1.000000\n",
            "[\n",
            "1.000000,\t1.000000,\t1.000000,\t1.000000;\n",
            "-114.309998,\t-114.470001,\t-114.559998,\t-114.570000;\n",
            "34.189999,\t34.400002,\t33.689999,\t33.639999;\n",
            "15.000000,\t19.000000,\t17.000000,\t14.000000;\n",
            "5612.000000,\t7650.000000,\t720.000000,\t1501.000000;\n",
            "1283.000000,\t1901.000000,\t174.000000,\t337.000000;\n",
            "1015.000000,\t1129.000000,\t333.000000,\t515.000000;\n",
            "472.000000,\t463.000000,\t117.000000,\t226.000000;\n",
            "1.493600,\t1.820000,\t1.650900,\t3.191700\n",
            "]\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606690,\t0.071937,\t-0.791307,\t-0.887247;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "iter: 0, logloss: 0.000000, accuracy: 0.000000\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606690,\t0.071937,\t-0.791307,\t-0.887247;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "iter: 1, logloss: 0.000000, accuracy: 0.000000\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "iter: 2, logloss: 0.000000, accuracy: 0.000000\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "iter: 3, logloss: 0.000000, accuracy: 0.000000\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "iter: 4, logloss: 0.000000, accuracy: 0.000000\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "iter: 5, logloss: 0.000000, accuracy: 0.000000\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "iter: 6, logloss: 0.000000, accuracy: 0.000000\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "iter: 7, logloss: 0.000000, accuracy: 0.000000\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "iter: 8, logloss: 0.000000, accuracy: 0.000000\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "[\n",
            "nan,\tnan,\tnan,\tnan;\n",
            "1.606672,\t0.071919,\t-0.791325,\t-0.887265;\n",
            "0.647916,\t1.295843,\t-0.894747,\t-1.049011;\n",
            "-0.650945,\t1.432078,\t0.390567,\t-1.171700;\n",
            "0.607537,\t1.318614,\t-1.099324,\t-0.826827;\n",
            "0.509270,\t1.385342,\t-1.062840,\t-0.831772;\n",
            "0.802337,\t1.144908,\t-1.247079,\t-0.700167;\n",
            "0.996940,\t0.938104,\t-1.323805,\t-0.611238;\n",
            "-0.807573,\t-0.324317,\t-0.574681,\t1.706571\n",
            "]\n",
            "iter: 9, logloss: 0.000000, accuracy: 0.000000\n",
            "Duration (s): 0.412770\n",
            "final accuracy: 0.000000\n",
            "final weights: \n",
            "[\n",
            "nan,\tnan;\n",
            "nan,\tnan;\n",
            "nan,\tnan;\n",
            "nan,\tnan;\n",
            "nan,\tnan;\n",
            "nan,\tnan;\n",
            "nan,\tnan;\n",
            "nan,\tnan;\n",
            "nan,\tnan\n",
            "]\n",
            "========= ERROR SUMMARY: 0 errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Pk-JL7rAhi"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqs64V1tEysf"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}